线性模型的基本形式：给定由d个属性描述的示例X=（x1，x2，x3，...,xd），线性模型试图学得一个通过属性的线性组合来进行预测的函数，即 F（X）=w1x1+w2x2+......+wdxd+b
一般用向量形式写成：F（X）=wx+b，其中w=（w1，w2，......,wd）
许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或者高维映射而得到，由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性，例如若在西瓜问题中：F好瓜(x)=0.2x色泽+0.5x根蒂+0.3x敲声+1，可以看出根蒂是最重要的，而敲声比色泽更重要。
线性回归：给定数据集D={(x1,y1),(x2,y2),......,(xn,yn)},其中xi=（x1,x2,......,xd）,线性回归试图学得一个线性模型以尽可能地准确预测实值的输出
最小二乘法：试图找到一条直线，使所有样本到直线的欧式距离之和最小
多元线性回归
广义线性回归
逻辑回归（对数几率函数）：逻辑回归函数是一种“Sigmoid”函数（形似S的函数，具有上下界），它将z值转化为一个接近0或者1的y值
逻辑回归求参数：假设样本服从伯努利分布，通过极大化似然函数计算，在运用梯度下降法求函数参数
损失函数：一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数
梯度下降算法本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式
线性判别分析（LDA）：是一种经典的线性学习方法，算法基本思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离，再根据投影点的位置来确定新样本的类别，现阶段也经常被用来做降维工作。
多分类学习：一般我们都是利用二分类学习器来解决多分类的问题
思想：考虑N个类别C1，C2，...,Cn,多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解，具体来说，先对问题进行拆分，然后为拆分的每个二分类任务训练一个分类器，在测试时对这些分类器的预测结果进行集成以获得最终的分类结果。
最经典的拆解方法
1、一对一（One vs One）：将这N个类别两两配对，从而产生N（N-1）/2个分类的任务
2、一对其余（One vs Rest）：每次将一个类的样例作为正例，所有其他类的样例作为反例来训练N个分类器，在测试时若仅有一个分类器预测正类，则对应的类别标记为最终分类结果，若多个分类器预测正类，则考虑置信度最大的类别标记作为最终结果
3、多对多（Many vs Many）：每次将若干个类作为正类，若干个其他类作为反类，显然，OvO和OvR是MvM的特例，MvM的正反类构造必须有特殊的设计，这里介绍一种最常见的MvM技术：“纠错输出码”
类别不平衡的问题：分类任务中不同种类的样例数目差别很大，处理类别不平衡的方法―再缩放
大体上有三类做法：1、直接对训练集的反类样例进行“欠采样”，即去除一些反例使得正反例数目接近
2、对训练集的正类样例进行“过采样”，即增加一些正例使得正反例数目接近
3、直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将再缩放嵌入到其决策过程中，称为“阈值移动”
SOMTE算法就是过采样算法的代表
