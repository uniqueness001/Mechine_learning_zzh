线性模型的基本形式：给定由d个属性描述的示例X=（x1，x2，x3，...,xd），线性模型试图学得一个通过属性的线性组合来进行预测的函数，即 F（X）=w1x1+w2x2+......+wdxd+b
一般用向量形式写成：F（X）=wx+b，其中w=（w1，w2，......,wd）
许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或者高维映射而得到，由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性，例如若在西瓜问题中：F好瓜(x)=0.2x色泽+0.5x根蒂+0.3x敲声+1，可以看出根蒂是最重要的，而敲声比色泽更重要。
线性回归：给定数据集D={(x1,y1),(x2,y2),......,(xn,yn)},其中xi=（x1,x2,......,xd）,线性回归试图学得一个线性模型以尽可能地准确预测实值的输出
最小二乘法：试图找到一条直线，使所有样本到直线的欧式距离之和最小
多元线性回归
广义线性回归
逻辑回归（对数几率函数）：逻辑回归函数是一种“Sigmoid”函数（形似S的函数，具有上下界），它将z值转化为一个接近0或者1的y值
逻辑回归求参数：假设样本服从伯努利分布，通过极大化似然函数计算，在运用梯度下降法求函数参数
损失函数：一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数
梯度下降算法本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式
线性判别分析（LDA）：是一种经典的线性学习方法，算法基本思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离，再根据投影点的位置来确定新样本的类别，现阶段也经常被用来做降维工作。
